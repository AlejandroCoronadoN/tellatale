{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809a94c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import urllib.request as ur\n",
    "import ipywidgets as widgets\n",
    "from config.configuration import SETTINGS\n",
    "\n",
    "from openai import OpenAI\n",
    "from IPython.display import IFrame, HTML\n",
    "os.environ[\"OPENAI_API_KEY\"] = SETTINGS.openai_api_key\n",
    "from config.configuration import SETTINGS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65272af",
   "metadata": {},
   "source": [
    "<h3> Creating a client object to call the APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
    "    # or you can explicitly pass in the key (NOT RECOMMENDED)\n",
    "    api_key=os.getenv(\"OPENAI_KEY\"),\n",
    ")\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f27b5a",
   "metadata": {},
   "source": [
    "<h3> The GPT models: </h3>\n",
    "    \n",
    "Here is an example to call the completions API and check that the key is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d174f5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo-1106\",\n",
    "  messages=[\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "         \"content\": \"You are a skilled Python programmer.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Generate a function that adds 2 numbers from the prompt.\"\n",
    "    }\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(response.model_dump()['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598bb980",
   "metadata": {},
   "source": [
    "<h4>Getting information about the tokens consumption for this request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e82e0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.model_dump()['usage'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d0e183",
   "metadata": {},
   "source": [
    "You can compute the cost of each call using the `chat.compleations` API with the following formula: `((promt_tokens * <cost_of_input_tokens>) + (completion_tokens * <cost_of_output_tokens>)) / 1000`\n",
    "\n",
    "For instance, this request using  `gpt-3.5-turbo-1106` the cost is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe46586a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_calculator_for_GPT_3_5_turbo(response):\n",
    "\n",
    "    # These 2 values are valid only for the \"gpt-3.5-turbo-1106\" model.\n",
    "    # Check https://openai.com/pricing for up-to-date prices\n",
    "    cost_of_input_tokens = 0.001\n",
    "    cost_of_output_tokens = 0.002\n",
    "\n",
    "    completion_tokens = response.model_dump()['usage']['completion_tokens']\n",
    "    prompt_tokens = response.model_dump()['usage']['prompt_tokens']\n",
    "\n",
    "    total_cost = (\n",
    "        (prompt_tokens * cost_of_input_tokens) + (completion_tokens * cost_of_output_tokens)\n",
    "    ) / 1000\n",
    "\n",
    "    return f\"Total cost for API call: ${total_cost} USD\"\n",
    "\n",
    "cost_calculator_for_GPT_3_5_turbo(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3899f9",
   "metadata": {},
   "source": [
    "<h4>Formatting the output generated from the API</h4>\n",
    "\n",
    "You can use `response_format` argument from the `chat.completions` API to structure the data generated by the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080786c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Can you generate json with information from your favorite actor?\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    "    response_format={\"type\": \"json_object\"},\n",
    ")\n",
    "print(cost_calculator_for_GPT_3_5_turbo(response))\n",
    "print(response.model_dump()['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dad0ab9",
   "metadata": {},
   "source": [
    "<h4>Mind the temeperature!</h4>\n",
    "\n",
    "You can use `temperature` to spice up the responses you get back from the models, but keep in mind that if you increase the `temperature` too much, things might not make much sense..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c232d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Can you generate json with information from your favorite actor?\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    "    response_format={\"type\": \"json_object\"},\n",
    "    temperature=1.9,\n",
    ")\n",
    "print(cost_calculator_for_GPT_3_5_turbo(response))\n",
    "print(response.model_dump()['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c7435f",
   "metadata": {},
   "source": [
    "<h4>Multiple answers and limiting tokens for the generated output</h4>\n",
    "\n",
    "You can use `n` argument to set the number of answers you want to get from the input prompt. While the `max_tokens` will limit the lenght of the answers generated by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9560b06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Is Pluto a planet?\"}],\n",
    "    temperature=1.5,\n",
    "    n=3,\n",
    "    max_tokens=20,\n",
    ")\n",
    "\n",
    "print(response.model_dump()['usage'], '\\n')\n",
    "print(cost_calculator_for_GPT_3_5_turbo(response), '\\n')\n",
    "for i in response.model_dump()['choices']:\n",
    "    print(f\"\"\"Option {i['index'] + 1}:\n",
    "\n",
    "    {i['message']['content']}\n",
    "\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc118cfe",
   "metadata": {},
   "source": [
    "Note that for the previous example we set `n=3` and `max_tokens=20`, but the `completion_tokens` value was 60! \n",
    "\n",
    "This means that each one of the 3 outputs contains `20 tokens`, that is why the total amount of output tokens was 60. It is also worth note that even if each answer contains of 20 tokens, the strings that they represent have different lenghts.\n",
    "\n",
    "\n",
    "\n",
    "<h3>Embeddings</h3>\n",
    "\n",
    "Here you will see how to create embeddings for any string you send as `input` to the `ada 2` model.\n",
    "\n",
    "Note that the dimension of the embeddings is currently fixed to 1536 by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7867f46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response = client.embeddings.create(\n",
    "    input=\"I am going to convert this into a vector!\",\n",
    "    model=\"text-embedding-ada-002\"\n",
    ")\n",
    "embeddings = response.model_dump()['data'][0]['embedding']\n",
    "print(\"Dimension of the vector:\", len(embeddings))\n",
    "embeddings[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9c9ff6",
   "metadata": {},
   "source": [
    "Following a classic example of word semantics being (more or less) preserved by the embeddings representing them, we can see how much of the relationships between the words is preserved through vector operations.\n",
    "\n",
    "For instance, using the set of words: `Queen`, `King`, `Woman` and `Man`, and by looking at their embeddings in 2D from the sample image below, one could think that the following operation should hold: `king + woman − man ≈ approx_queen`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eff5e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_img = (\n",
    "    \"https://www.researchgate.net/profile/Peter-Sutor/publication/\"\n",
    "    \"332679657/figure/fig1/AS:809485488640000@1570007788866/\"\n",
    "    \"The-classical-king-woman-man-queen-example-of-neural-word-embeddings-in-2D-It.png\"\n",
    ")\n",
    "IFrame(src=embedding_img, width=800, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c876e0e",
   "metadata": {},
   "source": [
    "So, let us see if the embeddings generated by the `ada 2` model hold some of these relationships.\n",
    "\n",
    "First, we will compute the embeddings for each of this words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe75e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.embeddings.create(\n",
    "    input=\"Man\",\n",
    "    model=\"text-embedding-ada-002\"\n",
    ")\n",
    "man_embedding = np.array(response.model_dump()['data'][0]['embedding'])\n",
    "\n",
    "response = client.embeddings.create(\n",
    "    input=\"King\",\n",
    "    model=\"text-embedding-ada-002\"\n",
    ")\n",
    "king_embedding = np.array(response.model_dump()['data'][0]['embedding'])\n",
    "\n",
    "response = client.embeddings.create(\n",
    "    input=\"Woman\",\n",
    "    model=\"text-embedding-ada-002\"\n",
    ")\n",
    "woman_embedding = np.array(response.model_dump()['data'][0]['embedding'])\n",
    "\n",
    "response = client.embeddings.create(\n",
    "    input=\"Queen\",\n",
    "    model=\"text-embedding-ada-002\"\n",
    ")\n",
    "queen_embedding = np.array(response.model_dump()['data'][0]['embedding'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae95de3",
   "metadata": {},
   "source": [
    "Once we have the embeddings for each word, we can proceed to compute the `approx_queen` embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ae475f",
   "metadata": {},
   "outputs": [],
   "source": [
    "approx_queen = king_embedding + woman_embedding - man_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5467df8",
   "metadata": {},
   "source": [
    "To see how close the `approx_queen` is to the `queen_embedding`, we can compute the `cosine similarity` of the 2 vectors with the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f234082",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e486551",
   "metadata": {},
   "source": [
    "Remember that the closer the value to 1 the more similar the vectors will be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4c803f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(queen_embedding, queen_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9e80ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(approx_queen, queen_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa81815d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cosine_similarity(approx_queen, man_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b5eb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(approx_queen, woman_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb6b33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(approx_queen, king_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019935d3",
   "metadata": {},
   "source": [
    "So, after performing all the comparisons, we can see that the `approx_queen` embedding is closer to `king_embedding`, although it the is the most distant to the `man_embedding`.\n",
    "\n",
    "\n",
    "<h3> Images with the Dall-E models</h3>\n",
    "\n",
    "First, you will use the `dall-e-3` model to generate images from a prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e9c47e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "image_size = 1024\n",
    "\n",
    "response = client.images.generate(\n",
    "    model=\"dall-e-3\",\n",
    "    prompt=\"\"\"\n",
    "    Everything should be painted with the Van Gogh style.\n",
    "    A dinosaur playing guitar with flames in the background.\n",
    "    \"\"\",\n",
    "    size=f\"{image_size}x{image_size}\"\n",
    ")\n",
    "\n",
    "image_url = response.model_dump()['data'][0]['url']\n",
    "IFrame(src=image_url, width=image_size, height=image_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673376dd",
   "metadata": {},
   "source": [
    "<h4>Image edition</h4>\n",
    "\n",
    "\n",
    "The `edit` API lets you modify an `image` using a `mask` from the same image.\n",
    "\n",
    "For this part we will need to load a couple of images from the `resources` directory, so make sure to update the path accordingly to your setup!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896df435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please change this path to match your directory structure\n",
    "media_path = \"resources/\"\n",
    "\n",
    "chihuahua = open(media_path + \"chihuahua.png\", \"rb\")\n",
    "chihuahua_mask = open(media_path + \"chihuahua_mask.png\", \"rb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03a6661",
   "metadata": {},
   "source": [
    "Please note that only the transparent areas from the `mask` will be used for editing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3291fe23",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 256\n",
    "\n",
    "response = client.images.edit(\n",
    "    image=chihuahua,\n",
    "    mask=chihuahua_mask,\n",
    "    prompt=\"\"\"\n",
    "    The chihuahua is on a trail in Hawaii with an active volcano in the background.\n",
    "    \"\"\",\n",
    "    n=1,\n",
    "    response_format='url',\n",
    "    size=f\"{image_size}x{image_size}\"\n",
    ")\n",
    "\n",
    "image_url = response.model_dump()['data'][0]['url']\n",
    "chihuahua_edit = ur.urlretrieve(image_url)[0]\n",
    "IFrame(src=image_url, width=image_size, height=image_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49abd67",
   "metadata": {},
   "source": [
    "You can see below the original `image` and the `mask` used in the `edit` call from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c01d885",
   "metadata": {},
   "outputs": [],
   "source": [
    "chihuahua.seek(0)\n",
    "chihuahua_mask.seek(0)\n",
    "\n",
    "img1=chihuahua.read()\n",
    "wi1 = widgets.Image(value=img1, format='jpg', width=image_size, height=image_size)\n",
    "img2=chihuahua_mask.read()\n",
    "wi2 = widgets.Image(value=img2, format='jpg', width=image_size, height=image_size)\n",
    "img3=open(chihuahua_edit, 'rb').read()\n",
    "wi3 = widgets.Image(value=img3, format='jpg', width=image_size, height=image_size)\n",
    "\n",
    "box=[wi1,wi2,wi3]\n",
    "chihuahuas=widgets.HBox(box)\n",
    "display(chihuahuas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8fbce9",
   "metadata": {},
   "source": [
    "<h4>Image variation</h4>\n",
    "\n",
    "This will let you create a random variation of the original `image` provided to the API call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c971fc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 256\n",
    "\n",
    "response = client.images.create_variation(\n",
    "    image=chihuahua,\n",
    "    n=1,\n",
    "    response_format='url',\n",
    "    size=f\"{image_size}x{image_size}\"\n",
    ")\n",
    "\n",
    "image_url = response.model_dump()['data'][0]['url']\n",
    "IFrame(src=image_url, width=image_size, height=image_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4477bfed",
   "metadata": {},
   "source": [
    "<h2>Audio with Whisper</h2>\n",
    "\n",
    "`whisper 1` will let you generate `transcriptions` from the audio files provided. By specifying the orignal language of the audio, you can help the model to get a faster and more accurate result!\n",
    "\n",
    "Make sure to check the limits for duration and file size in the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beaab82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_audio_with_controls(file_path):\n",
    "    display(HTML(\"<audio controls><source src={} type='audio/mpeg'></audio>\".format(file_path)))\n",
    "\n",
    "english_audio_file_path = media_path + \"english_audio.mp3\"\n",
    "\n",
    "show_audio_with_controls(english_audio_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb47a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript = client.audio.transcriptions.create(\n",
    "    model=\"whisper-1\",\n",
    "    file=open(english_audio_file_path, \"rb\"),\n",
    "    language=\"en\"\n",
    ")\n",
    "\n",
    "transcript.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6c0898",
   "metadata": {},
   "source": [
    "Note that this API also lets you play with the temperature, so you can see how much the transcription changes as you play with this parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c321e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript = client.audio.transcriptions.create(\n",
    "    model=\"whisper-1\",\n",
    "    file=open(english_audio_file_path, \"rb\"),\n",
    "    language=\"en\",\n",
    "    temperature=1.3,\n",
    ")\n",
    "transcript.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db37903",
   "metadata": {},
   "source": [
    "<h4>Translations</h4>\n",
    "\n",
    "This API will help you to generate `translations` from any language in the list of supported languages by Open AI, to a text in English.\n",
    "\n",
    "You can test it with the following audio in Spanish:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf72a6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_audio_file_path = media_path + \"opiniones.mp3\"\n",
    "\n",
    "show_audio_with_controls(spanish_audio_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b3239d",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file= open(spanish_audio_file_path, \"rb\")\n",
    "translation = client.audio.translations.create(\n",
    "  model=\"whisper-1\",\n",
    "  file=audio_file\n",
    ")\n",
    "\n",
    "translation.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1872e67",
   "metadata": {},
   "source": [
    "<h3>Text To Speech with TTS 1</h3>\n",
    "\n",
    "Pretty straightforward, type in whatever you want to say, choose the type of voice from the selection, point to the file where you want to store the audio and enjoy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1858c386",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_speech_file = media_path + \"generated_speech.mp3\"\n",
    "\n",
    "response = client.audio.speech.create(\n",
    "  model=\"tts-1\",\n",
    "  voice=\"alloy\",\n",
    "  input=\"Look mom, I am coding in Python!\"\n",
    ")\n",
    "\n",
    "response.stream_to_file(output_speech_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f354ad60",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_audio_with_controls(output_speech_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatgpt-sprint",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "fd3f83c403a6095c93d728edcaf2ffe085649c04d3e97d57cd773ee4e26b7705"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
